Automatic text summarization is a process that takes a source text and presents the most important content in a condensed form in a manner sensitive to the user or task needs. The system simply needs to identify the most important passages of the text to produce an extract. We can compare a system summary to the source text, to a human-generated summary or to another system summary. In extrinsic evaluation, the summary quality is judged on the basis of how helpful summaries are for a given task, and in intrinsic evaluation, it is directly based on analysis of the summary. The latter can involve a comparison with the source document, measuring how many main ideas of the source document are covered by the summary or a content comparison with an abstract written by a human. Whereas content evaluations measure the ability to identify the key topics, text quality evaluations judge the readability, grammar and coherence of automatic summaries. It was found that the addition of anaphoric knowledge leads to improved performance of the summarizer. We present here a summary evaluation method whose idea is that the summary should retain the main topics of the source text. In Section 5 we propose our LSA-based evaluation method. The experimental part (Section 6) covers a comparison of 13 summarization systems that participated in DUC 20021 from the point of view of several evaluation measures: two baselines, the standard ROUGE measure (see Section 3.3.4) and our proposed LSA measures.